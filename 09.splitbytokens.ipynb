{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 按令牌拆分"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "171cc143d1f49207"
  },
  {
   "cell_type": "markdown",
   "source": [
    "语言模型具有标记限制。您不应超过令牌限制。因此，当您将文本拆分为块时，最好计算标记的数量。有许多分词器。在计算文本中的标记时，应使用与语言模型中使用的相同的标记器。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe75b304e9644a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tiktoken\n",
    "Tiktoken 是由 OpenAI 创建的快速 BPE 分词器。\n",
    "\n",
    "我们可以用它来估计使用的token。对于 OpenAI 模型来说，它可能会更准确。\n",
    "\n",
    "1. 文本的拆分方式：按传入的字符。\n",
    "2. 如何测量块大小：通过 tiktoken 分词器。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efee475771324be1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./files/liudehua.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:23:28.160203800Z",
     "start_time": "2024-04-07T14:23:27.982353600Z"
    }
   },
   "id": "db58d3b630eb0e0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "该 .from_tiktoken_encoder() 方法要么 encoding 作为参数（例如 cl100k_base ），要么采用（ model_name 例如 gpt-4 ）。所有其他参数（如 chunk_size 、 chunk_overlap 和 separators ）都用于实例化 CharacterTextSplitter ："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf574e6a4db71e11"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=5000, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T07:45:08.407553900Z",
     "start_time": "2024-04-07T07:45:08.390149600Z"
    }
   },
   "id": "79f2f7fdff7d7ea6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘德华，BBS，MH，JP（英语：Andy Lau Tak Wah；1961年9月27日—），香港男演员、歌手、填词人、监制及出品人，1990年代获封为香港乐坛“四大天王”之一[3]，也是吉尼斯世界纪录大全中获奖最多的香港歌手[4]；在影视方面，他三次获得香港电影金像奖最佳男主角奖，两次获得金马奖最佳男主角奖，至今参演电影超过170部[5]。刘德华是天幕公司和映艺集团的创建者，作为投资人与监制已参与制作了30多部华语电影[6]。除此之外，刘德华是四川省川剧学校客座教授[7]。\n",
      "\n",
      "1999年，刘德华获得“香港十大杰出青年”的荣誉，2000年11月则顺利荣登“世界十大杰出青年”[8]，成为获此殊荣的少数几位香港艺人。2006年7月7日，香港演艺学院因他“是香港最受尊重和喜爱的演艺名人之一，对香港电影及音乐贡献良多。其严谨专业的工作态度，足以成为年轻人的典范”，为了“表彰他在表演艺术方面的成就”而授予刘德华荣誉院士称号[9]，他也因此成为少数几位获此荣誉的香港艺人之一[10]。\n",
      "\n",
      "刘德华笃信佛教，法号“慧果”，热心公益，时常参与慈善活动。2008年，刘德华获香港特别行政区政府委任为太平绅士[11]，2010年4月23日，刘德华获任中国残疾人福利基金会理事并担任副理事长[12]。2010年5月2日，刘德华获颁第十二届“世界杰出华人奖”同时获颁授加拿大纽奔驰域蓝仕桥大学荣誉博士学位[13]。2013年12月8日，他又当选香港残疾人奥委会暨伤残人士体育协会副会长[14]。2017年12月他因演艺事业和公益事务上的成就被香港树仁大学授予荣誉文学博士学位。2018年6月受邀加入美国电影艺术与科学学会成为会员[15]。\n",
      "\n",
      "\n",
      "刘德华出生于香港新界大埔泰亨村（旧称菜坑村[注 1]）[16]，籍贯广东新会县荷塘镇（今江门市）[17]。祖父在当时算是大地主（乡村、农地）[18]。另外，他在家中亦有三姊、一妹和一弟（刘德盛）[19][20][21]，自己在家中排行第四[22]。其父刘礼年轻时为启德机场的消防员[16]。\n",
      "\n",
      "刘德华五岁多时由于任职驻守机场消防员的父亲希望他能入读英文小学而随家人离开了农村[注 1]，全家后来搬到了九龙钻石山大磡村（曾就读村里的大磡村街坊福利会小学）[23]。钻石山为贫民区，多是木屋，容易发生火灾。刘家的木屋在刘德华十一岁时被大火烧毁，家人因此住在寮屋一年[18]，家人后来再搬到蓝田邨第十五座。[24]在刘德华五、六岁时，父亲还开了一间名叫“得胜士多”的小吃杂货店以赚钱维持家用[注 1]；刘德华与姐姐们则经常担任店里的帮工一起干活，当时负责写菜名的他为日后写下一手好的毛笔字奠定了基础[25]。离店不远处有一家“坚城片场”，往片场送外卖的他也能常见到曹达华、石坚、冯宝宝等当红演员拍戏时的模样[26]。因为父亲嗜吃叉烧，后来刘德华更把蓝田邨十五座地下的“华东烧腊”买下来送予父亲，现时他和父母都住在加多利山大宅[27]，而其大姐一家则住在大围新翠邨公屋。[28]\n",
      "\n",
      "刘德华，出生时取名“德华”，在就学阶段曾取学名为“福荣”，刘福荣只是学名而非本名，“刘德华”就是本名[29]，他本人在《鲁豫有约》、《康熙来了》中曾亲口公开澄清此事。[30][31] 刘德华在黄大仙天主教小学毕业后[32][注 1]，升读位于新蒲岗的英文中学可立中学[33]。升读可立之后，刘德华于就读中一级时曾因英文科成绩差而留级一年[注 1]，经补习后继续升班[注 1]。与校内要好的同学走在一起，还自号“可立七侠”[25]，同时，他参加校外中小学跳弹床公开比赛[注 2]。亦热心参加校内外学校剧社的表演[注 2]，参与幕后制作负责编剧[注 2]，而教授他有关戏剧方面知识的地理科老师，就是后来的著名舞台剧编剧杜国威[33][注 2]。刘德华在中五会考获得1B3D2E（中文读本A）的成绩，中六上学期后，到香港电视广播有限公司（TVB）的艺员训练班受训[34]。此前他在中学阶段开始到慈云山踢足球[注 2]，与黄日华识于微时[注 2]；当时刘担任守门员[注 2]，黄担任前线球员[注 2]。\n",
      "\n",
      "1980年刘德华为了想当导演而入读第10期无线电视艺员训练班[35][36][注 3]，著名的同期同学有吴家丽、梁家辉、戚美珍、张之亮、徐锦江等，1981年毕业后任无线电视台演员[34]。在香港电台电视部制作的电视单元剧《香港香港8：江湖再见》里首次演出[37]。其后在李添胜监制的时装警匪电视剧《猎鹰》里首度担演男主角饰演一名投身警界的青年警察而开始走红[35]。\n",
      "\n",
      "1983年受TVB力捧，与黄日华、梁朝伟、苗侨伟和汤镇业组成“无线五虎将”[35]，相继出演了1983年萧笙监制、与陈玉莲合作的50集武侠剧《神雕侠侣》，1984年李添胜监制、与梁朝伟合作的《鹿鼎记》等多部很受欢迎的无线剧集，其中在片中饰演杨过的《神雕侠侣》不仅是刘德华最为重要的电视剧代表作[38]，其人气甚至让他成为电影投资方，导演等重用的新晋，奠定他往影坛发展基础。1999年该剧为刘德华赢得TVB无线千禧“我最难忘的男主角”荣誉，而且还被金庸先生于2003年选为他最满意的根据其小说改编成的两部电视剧之一（另外一部是1976年郑少秋主演《书剑恩仇录》）[39]。\n",
      "\n",
      "刘德华于1982年开始参演电影，首部作品是吴小云执导的《彩云曲》[35]，其中刘的戏份比较少。第一部参演的重要电影作品是1982年许鞍华执导的表现越南战后普通人生活和移民香港题材的《投奔怒海》[37]，获得第二届香港电影金像奖最佳新演员提名，不过败于以同一电影提名的女演员马斯晨[40]。1983年霍耀良执导的《毁灭号地车》是刘德华首部担纲男主角的电影。\n",
      "\n",
      "1985年吴思远执导、刘德华与叶德娴主演的法庭剧情片《法外情》是刘德华早期主演电影中的一部重要代表作[41]。刘在片中扮演一位年轻的律师在法庭上唇枪舌战的表现不俗，而法庭外与身世有关的感情戏则比较赚人眼泪。《法外情》的成功促使主创们后来又拍摄了两部续集《法内情》和《法内情大结局》[42]。另外，这时期刘跟洪金宝等人合作的群星动作片《最佳福星》，以及与周润发合作的江湖片《江湖情》都获取了良好的票房成绩。\n",
      "\n",
      "1988年至1992年是刘德华拍片量最多的时期，这五年他参演了超过50部作品。其中演出最多最成功的类型是黑社会江湖片，并塑造了多个成长于草根、身在江湖却有情有义英雄未泯、却以悲剧收场的“情深烂仔”、“良心古惑仔”与“悲剧英雄”角色，深深影响当时的年轻人[43][44]。1988年他与张曼玉和张学友合作王家卫的导演处女作《旺角卡门》，塑造了一个重情重义的江湖混混华仔形象，使其首次获得香港电影金像奖最佳男主角提名[45]，该片也为刘赢得首座表演奖杯——台湾金龙表演艺术奖最佳演员奖[35]；1989年由向华胜、王晶联合执导，刘与谭咏麟合作的黑帮赌片《至尊无上》票房达2300多万港币，是为数不多的讲述兄弟情义的港产赌片。1990年他与吴倩莲在陈木胜的导演处女作《天若有情》中，演绎了小混混华Dee与千金小姐JOJO之间一段刻骨铭心的浪漫生死恋，感动了华语地区及韩国等地许多观众[46]，本片的成功也导致后来他与吴倩莲在杜琪峰执导的1991年作品《至尊无上II之永霸天下》和1996年作品《天若有情III之烽火佳人》中合演情侣[47]。该时期他主演的其他主要古惑仔电影还有《同根生》、《飚城》、《狱中龙》等。\n",
      "\n",
      "而刘德华亦凭借其出众的偶像外型拍摄了大量的商业片，其中与著名导演王晶合作的次数最多，双方合作最具代表性的作品有以《最佳损友》系列、《精装追女仔II》、《与龙共舞》为代表的爱情喜剧片，刘德华在其中表现了喜剧表演才能，票房成绩也不俗；他与王晶合作的另一类主要电影是赌博片，如1989年与周润发合作的《赌神》，1990年与周星驰合作的《赌神》续集《赌侠》，都成为当时非常卖座的商业类型片[43][48]，这些作品使得刘德华的“赌侠”与周润发的“赌神”和周星驰的“赌圣”并列为港产赌片的三大代表角色[48]。90年代后期刘与王晶又合作了两部以King哥为主角的续集《赌侠1999》和《赌侠大战拉斯维加斯》，分别于1998年与1999年上映。[48]\n",
      "\n",
      "此外，该时期比较知名的作品还有《九一神雕侠侣》和《五亿探长雷洛传》。1991年的《九一神雕侠侣》是刘德华成立的天幕制作有限公司拍摄的首部作品，是他与梅艳芳等人合作的一部现代奇幻浪漫动作片，收获了良好的反响和票房[49]。同年分为两部先后上映的《五亿探长雷洛传》由刘国昌执导，是一部改编自真人故事的传记片，刻画了一个香港警察雷洛在腐败横行的法治乱世中成长为巨贪枭雄的兴衰成败史[45]，两部累计票房5300万，刘德华也凭借雷洛一角获得第11届香港电影金像奖最佳男主角提名[45]。《雷洛传》的成功使刘德华又接拍了表现澳门赌王传记题材的《赌城大亨》系列，也分成两部于1992年先后上映，获得不错反响[43][48]。不过，高产时期的刘德华也往往被认为“处理性格和情感没有深度，角色类型化”，影评人张伟雄说：“他喜欢在镜头里玩形态，玩火机，玩枪，玩小动作，连发型都不能改。可以说，他是同一时期一直演两种角色而已，路数甚窄。多产期还造就了刘德华在表演上的另一个大特点，就是模仿。”[43]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T07:45:16.895524700Z",
     "start_time": "2024-04-07T07:45:16.888690100Z"
    }
   },
   "id": "e933fbd0de487d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "请注意，如果我们使用 CharacterTextSplitter.from_tiktoken_encoder ，文本仅被拆分， CharacterTextSplitter tiktoken 而 tokenizer 用于合并拆分。这意味着拆分可以大于分 tiktoken 割器测量的块大小。我们可以用来 RecursiveCharacterTextSplitter.from_tiktoken_encoder 确保拆分不大于语言模型允许的标记块大小，如果每个拆分的大小较大，则每个拆分都将递归拆分："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "655f079621573456"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘德华，BBS，MH，JP（英语：Andy Lau Tak Wah；1961年9月27日—），香港男演员、歌手、填词人、监制及出品人，1990年代获封为香港乐坛“四大天王”之一[3]，也是吉尼斯世界纪录大全中获奖最多的香港歌手[4]；在影视方面，他三次获得香港电影金像奖最佳男主角奖，两次获得金马奖最佳男主角奖，至今参演电影超过170部[5]。刘德华是天幕公司和映艺集团的创建者，作为投资人与监制已参与制作了30多部华语电影[6]。除此之外，刘德华是四川省川剧学校客座教授[7]。\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:23:47.017055700Z",
     "start_time": "2024-04-07T14:23:46.944811300Z"
    }
   },
   "id": "cded07b90740133a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们还可以直接加载一个 tiktoken 拆分器，这将确保每个拆分都小于块大小。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "897cd3ae13f2b007"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘德华，B\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T07:45:57.202208500Z",
     "start_time": "2024-04-07T07:45:57.110512Z"
    }
   },
   "id": "c0a2254bfca6f622"
  },
  {
   "cell_type": "markdown",
   "source": [
    "一些书面语言（例如中文和日文）具有编码为 2 个或更多标记的字符。直接使用 可以在 TokenTextSplitter 两个块之间拆分字符的标记，从而导致格式错误的 Unicode 字符。使用 RecursiveCharacterTextSplitter.from_tiktoken_encoder 或 CharacterTextSplitter.from_tiktoken_encoder 确保块包含有效的 Unicode 字符串。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64733daaa4100b0f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## spaCy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac2db48ff0d96fac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "自然语言工具包，或更常见的 NLTK，是一套用 Python 编程语言编写的用于英语符号和统计自然语言处理 （NLP） 的库和程序。\n",
    "我们不仅可以在“”上拆分， NLTK 还可以根据 NLTK 分词器进行拆分。\n",
    "1. 如何拆分文本：通过 NLTK 分词器。\n",
    "2. 如何测量块大小：按字符数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a4466ceb2bb5e3"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./files/liudehua.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:27:31.045999800Z",
     "start_time": "2024-04-07T14:27:31.040985600Z"
    }
   },
   "id": "aea380247e2d8a9b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:27:31.460204200Z",
     "start_time": "2024-04-07T14:27:31.043493400Z"
    }
   },
   "id": "f45de4f524056168"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\thinkpad\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "nltk.download('popular')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-07T14:27:32.750980300Z"
    }
   },
   "id": "38886c456d658e76"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\thinkpad/nltk_data'\n    - 'D:\\\\anaconda3\\\\envs\\\\learn-text-splitters\\\\nltk_data'\n    - 'D:\\\\anaconda3\\\\envs\\\\learn-text-splitters\\\\share\\\\nltk_data'\n    - 'D:\\\\anaconda3\\\\envs\\\\learn-text-splitters\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\thinkpad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m texts \u001B[38;5;241m=\u001B[39m \u001B[43mtext_splitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_of_the_union\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(texts[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-text-splitters\\Lib\\site-packages\\langchain_text_splitters\\nltk.py:30\u001B[0m, in \u001B[0;36mNLTKTextSplitter.split_text\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Split incoming text and return chunks.\"\"\"\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# First we naively split the large input into a bunch of smaller ones.\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m splits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_language\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_splits(splits, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_separator)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-text-splitters\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001B[0m, in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msent_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     97\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 106\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizers/punkt/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlanguage\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pickle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mtokenize(text)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-text-splitters\\Lib\\site-packages\\nltk\\data.py:750\u001B[0m, in \u001B[0;36mload\u001B[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[0;32m    747\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<<Loading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresource_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m>>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    749\u001B[0m \u001B[38;5;66;03m# Load the resource.\u001B[39;00m\n\u001B[1;32m--> 750\u001B[0m opened_resource \u001B[38;5;241m=\u001B[39m \u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresource_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    753\u001B[0m     resource_val \u001B[38;5;241m=\u001B[39m opened_resource\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-text-splitters\\Lib\\site-packages\\nltk\\data.py:876\u001B[0m, in \u001B[0;36m_open\u001B[1;34m(resource_url)\u001B[0m\n\u001B[0;32m    873\u001B[0m protocol, path_ \u001B[38;5;241m=\u001B[39m split_resource_url(resource_url)\n\u001B[0;32m    875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m protocol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnltk\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 876\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mopen()\n\u001B[0;32m    877\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    878\u001B[0m     \u001B[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001B[39;00m\n\u001B[0;32m    879\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m find(path_, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mopen()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\learn-text-splitters\\Lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\thinkpad/nltk_data'\n    - 'D:\\\\anaconda3\\\\envs\\\\learn-text-splitters\\\\nltk_data'\n    - 'D:\\\\anaconda3\\\\envs\\\\learn-text-splitters\\\\share\\\\nltk_data'\n    - 'D:\\\\anaconda3\\\\envs\\\\learn-text-splitters\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\thinkpad\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:24:31.405253300Z",
     "start_time": "2024-04-07T14:24:31.109935900Z"
    }
   },
   "id": "746574093f5f48e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KoNLPY"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72797e15ced0e3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "KoNLPy：Python 中的韩语 NLP 是一个用于韩语自然语言处理 （NLP） 的 Python 包。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcc46551379b51a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "标记拆分涉及将文本分割成更小、更易于管理的单元，称为标记。这些标记通常是单词、短语、符号或其他对进一步处理和分析至关重要的有意义的元素。在英语等语言中，标记拆分通常涉及用空格和标点符号分隔单词。代币拆分的有效性很大程度上取决于代币制作者对语言结构的理解，从而确保生成有意义的代币。由于为英语设计的分词器无法理解其他语言（如韩语）的独特语义结构，因此它们不能有效地用于韩语处理。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8de1e93e302eaef8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用 KoNLPy 的 Kkma 分析器对韩语进行 token 拆分"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e107aca628ec9a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在韩文文本的情况下，KoNLPY包括称为 Kkma （Korean Knowledge Morpheme Analyzer）的形态分析器。 Kkma 提供韩文文本的详细形态分析。它将句子分解为单词，将单词分解为各自的语素，识别每个标记的词性。它可以将文本块分割成单个句子，这对于处理长文本特别有用。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80f257f518fb39c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用注意事项\n",
    "虽然 Kkma 以其详细的分析而闻名，但需要注意的是，这种精度可能会影响处理速度。因此， Kkma 最适合分析深度优先于快速文本处理的应用。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b449c112e12a45c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is a long Korean document that we want to split up into its component sentences.\n",
    "with open(\"./your_korean_doc.txt\") as f:\n",
    "    korean_document = f.read()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dbe076bad3813ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "text_splitter = KonlpyTextSplitter()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5eda9e5b21d1ce8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(korean_document)\n",
    "# The sentences are split with \"\\n\\n\" characters.\n",
    "print(texts[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e3fc09469f58061"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hugging Face tokenizer\n",
    "\n",
    "Hugging Face 有许多标记器。\n",
    "我们使用 Hugging Face 分词器，即 GPT2TokenizerFast 来计算以令牌为单位的文本长度。\n",
    "\n",
    "1. 文本的拆分方式：按传入的字符。\n",
    "2. 如何测量块大小：通过 Hugging Face 分词器计算的令牌数量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41a55974e33d482e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b895b9233f068021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./files/liudehua.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2107898401d34b17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86694a4384e0d517"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(texts[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b87be4d92a9f3de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
